{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469cf5a8",
   "metadata": {},
   "source": [
    "#### Byte Pair Encoding \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "834fd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corpus = \"The process is called a Neurtalization where the mixture is normalized and process is called as Normalization\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f6272",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dcca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairTokenizer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = []\n",
    "        self.freq_vocab = dict()\n",
    "        self.normalized_corpus = list()\n",
    "\n",
    "\n",
    "    def letter_freq_count(self):\n",
    "        for word in self.normalized_corpus:\n",
    "            for char in word:\n",
    "                self.freq_vocab[char] = self.freq_vocab.get(char,0)+1\n",
    "        \n",
    "    def normalizer(self):\n",
    "        #split the words with space and add an underscore character at end\n",
    "        self.normalized_corpus = [\" \".join(list(word + \"_\")) for word in self.data_corpus.split()]\n",
    "    \n",
    "\n",
    "    def max_binary_pair(self):\n",
    "        pairs = dict()\n",
    "        for word in self.normalized_corpus:\n",
    "            chars = word.split()\n",
    "            for i in range(len(chars)-1):\n",
    "                pair = (chars[i],chars[i+1])\n",
    "                pairs[pair] = pairs.get(pair,0) + 1\n",
    "        return max(pairs, key=pairs.get)\n",
    "    \n",
    "    def freq_counter(self,char):\n",
    "        count = 0\n",
    "        for word in self.normalized_corpus:\n",
    "            symbols = word.split()\n",
    "            for sym in symbols:\n",
    "                if sym==char:\n",
    "                    count= count+1\n",
    "        return count\n",
    "    def update_freq_table(self,letter1,letter2,freq):\n",
    "        self.freq_vocab[letter1] = self.freq_vocab.get(letter1,0)-freq\n",
    "        self.freq_vocab[letter2] = self.freq_vocab.get(letter2,0)-freq\n",
    "        \n",
    "\n",
    "\n",
    "    def merge_pairs(self,pair):\n",
    "        pair_str = \" \".join(pair)\n",
    "        merged_symbol = \"\".join(pair)\n",
    "        for i in range(len(self.normalized_corpus)):\n",
    "            self.normalized_corpus[i] = self.normalized_corpus[i].replace(pair_str, merged_symbol)\n",
    "\n",
    "        #add the merged pair into vocabulary\n",
    "        self.vocab.append(merged_symbol)\n",
    "        #count the freq of merged symbol and add to freq table\n",
    "        pair_freq = self.freq_counter(merged_symbol)\n",
    "        self.freq_vocab[merged_symbol] = pair_freq\n",
    "        #update the freq_table\n",
    "        self.update_freq_table(pair[0],pair[1],pair_freq)\n",
    "        print(\"frequency_vocab:\",self.freq_vocab)\n",
    "        print(self.normalized_corpus)\n",
    "        print(\"vocaulary :\",self.vocab)\n",
    "\n",
    "      \n",
    "    def train(self,data_corpus):\n",
    "        self.data_corpus = data_corpus\n",
    "        self.normalizer()\n",
    "        #creating the letter pair {'a':23,'b':21}\n",
    "        self.letter_freq_count()\n",
    "\n",
    "        ####\n",
    "        # finding the max merge pair and merging it \n",
    "        for i in range(10):\n",
    "            result = self.max_binary_pair()\n",
    "            print(result)\n",
    "            self.merge_pairs(result)\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "557b6133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('s', '_')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 11, '_': 17, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 8, 'i': 9, 'a': 10, 'l': 7, 'd': 4, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6}\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 11, '_': 11, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 9, 'a': 10, 'l': 7, 'd': 4, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6}\n",
      "['T h e _', 'p r o c e s s_', 'i s_', 'c a l l e d _', 'a _', 'N e u r t a l i z a t i o n _', 'w h e r e _', 't h e _', 'm i x t u r e _', 'i s_', 'n o r m a l i z e d _', 'a n d _', 'p r o c e s s_', 'i s_', 'c a l l e d _', 'a s_', 'N o r m a l i z a t i o n _']\n",
      "('a', 'l')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 11, '_': 11, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 9, 'a': 10, 'l': 7, 'd': 4, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6, 'al': 5}\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 11, '_': 11, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 9, 'a': 5, 'l': 2, 'd': 4, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6, 'al': 5}\n",
      "['T h e _', 'p r o c e s s_', 'i s_', 'c al l e d _', 'a _', 'N e u r t al i z a t i o n _', 'w h e r e _', 't h e _', 'm i x t u r e _', 'i s_', 'n o r m al i z e d _', 'a n d _', 'p r o c e s s_', 'i s_', 'c al l e d _', 'a s_', 'N o r m al i z a t i o n _']\n",
      "('e', '_')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 11, '_': 11, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 9, 'a': 5, 'l': 2, 'd': 4, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6, 'al': 5, 'e_': 4}\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 7, '_': 7, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 9, 'a': 5, 'l': 2, 'd': 4, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6, 'al': 5, 'e_': 4}\n",
      "['T h e_', 'p r o c e s s_', 'i s_', 'c al l e d _', 'a _', 'N e u r t al i z a t i o n _', 'w h e r e_', 't h e_', 'm i x t u r e_', 'i s_', 'n o r m al i z e d _', 'a n d _', 'p r o c e s s_', 'i s_', 'c al l e d _', 'a s_', 'N o r m al i z a t i o n _']\n",
      "('d', '_')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 7, '_': 7, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 9, 'a': 5, 'l': 2, 'd': 4, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6, 'al': 5, 'e_': 4, 'd_': 4}\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 7, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 9, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6, 'al': 5, 'e_': 4, 'd_': 4}\n",
      "['T h e_', 'p r o c e s s_', 'i s_', 'c al l e d_', 'a _', 'N e u r t al i z a t i o n _', 'w h e r e_', 't h e_', 'm i x t u r e_', 'i s_', 'n o r m al i z e d_', 'a n d_', 'p r o c e s s_', 'i s_', 'c al l e d_', 'a s_', 'N o r m al i z a t i o n _']\n",
      "('i', 's_')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 7, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 9, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 6, 'al': 5, 'e_': 4, 'd_': 4, 'is_': 3}\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 7, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 6, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 5, 'e_': 4, 'd_': 4, 'is_': 3}\n",
      "['T h e_', 'p r o c e s s_', 'is_', 'c al l e d_', 'a _', 'N e u r t al i z a t i o n _', 'w h e r e_', 't h e_', 'm i x t u r e_', 'is_', 'n o r m al i z e d_', 'a n d_', 'p r o c e s s_', 'is_', 'c al l e d_', 'a s_', 'N o r m al i z a t i o n _']\n",
      "('e', 'd_')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 7, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 6, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 5, 'e_': 4, 'd_': 4, 'is_': 3, 'ed_': 3}\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 4, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 6, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 5, 'e_': 4, 'd_': 1, 'is_': 3, 'ed_': 3}\n",
      "['T h e_', 'p r o c e s s_', 'is_', 'c al l ed_', 'a _', 'N e u r t al i z a t i o n _', 'w h e r e_', 't h e_', 'm i x t u r e_', 'is_', 'n o r m al i z ed_', 'a n d_', 'p r o c e s s_', 'is_', 'c al l ed_', 'a s_', 'N o r m al i z a t i o n _']\n",
      "('al', 'i')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 4, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 6, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 5, 'e_': 4, 'd_': 1, 'is_': 3, 'ed_': 3, 'ali': 3}\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 4, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 3, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 2, 'e_': 4, 'd_': 1, 'is_': 3, 'ed_': 3, 'ali': 3}\n",
      "['T h e_', 'p r o c e s s_', 'is_', 'c al l ed_', 'a _', 'N e u r t ali z a t i o n _', 'w h e r e_', 't h e_', 'm i x t u r e_', 'is_', 'n o r m ali z ed_', 'a n d_', 'p r o c e s s_', 'is_', 'c al l ed_', 'a s_', 'N o r m ali z a t i o n _']\n",
      "('ali', 'z')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 4, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 3, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 3, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 2, 'e_': 4, 'd_': 1, 'is_': 3, 'ed_': 3, 'ali': 3, 'aliz': 3}\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 4, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 3, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 0, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 2, 'e_': 4, 'd_': 1, 'is_': 3, 'ed_': 3, 'ali': 0, 'aliz': 3}\n",
      "['T h e_', 'p r o c e s s_', 'is_', 'c al l ed_', 'a _', 'N e u r t aliz a t i o n _', 'w h e r e_', 't h e_', 'm i x t u r e_', 'is_', 'n o r m aliz ed_', 'a n d_', 'p r o c e s s_', 'is_', 'c al l ed_', 'a s_', 'N o r m aliz a t i o n _']\n",
      "('h', 'e_')\n",
      "{'T': 1, ' ': 93, 'h': 3, 'e': 4, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 3, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 0, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 2, 'e_': 4, 'd_': 1, 'is_': 3, 'ed_': 3, 'ali': 0, 'aliz': 3, 'he_': 2}\n",
      "{'T': 1, ' ': 93, 'h': 1, 'e': 4, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 3, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 0, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 2, 'e_': 2, 'd_': 1, 'is_': 3, 'ed_': 3, 'ali': 0, 'aliz': 3, 'he_': 2}\n",
      "['T he_', 'p r o c e s s_', 'is_', 'c al l ed_', 'a _', 'N e u r t aliz a t i o n _', 'w h e r e_', 't he_', 'm i x t u r e_', 'is_', 'n o r m aliz ed_', 'a n d_', 'p r o c e s s_', 'is_', 'c al l ed_', 'a s_', 'N o r m aliz a t i o n _']\n",
      "('p', 'r')\n",
      "{'T': 1, ' ': 93, 'h': 1, 'e': 4, '_': 3, 'p': 2, 'r': 7, 'o': 6, 'c': 4, 's': 2, 'i': 3, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 0, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 2, 'e_': 2, 'd_': 1, 'is_': 3, 'ed_': 3, 'ali': 0, 'aliz': 3, 'he_': 2, 'pr': 2}\n",
      "{'T': 1, ' ': 93, 'h': 1, 'e': 4, '_': 3, 'p': 0, 'r': 5, 'o': 6, 'c': 4, 's': 2, 'i': 3, 'a': 5, 'l': 2, 'd': 0, 'N': 2, 'u': 2, 't': 5, 'z': 0, 'n': 4, 'w': 1, 'm': 3, 'x': 1, 's_': 3, 'al': 2, 'e_': 2, 'd_': 1, 'is_': 3, 'ed_': 3, 'ali': 0, 'aliz': 3, 'he_': 2, 'pr': 2}\n",
      "['T he_', 'pr o c e s s_', 'is_', 'c al l ed_', 'a _', 'N e u r t aliz a t i o n _', 'w h e r e_', 't he_', 'm i x t u r e_', 'is_', 'n o r m aliz ed_', 'a n d_', 'pr o c e s s_', 'is_', 'c al l ed_', 'a s_', 'N o r m aliz a t i o n _']\n"
     ]
    }
   ],
   "source": [
    "tokenzier = BytePairTokenizer()\n",
    "tokenzier.train(data_corpus=data_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9609e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abd34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
