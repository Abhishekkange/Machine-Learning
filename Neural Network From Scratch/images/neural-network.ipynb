{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ddb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ecebc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_weights = np.random.randn(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5977d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2956938 ],\n",
       "       [ 0.43168711],\n",
       "       [-0.94712553]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e340d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "\n",
    "    def __init__(self,num_inputs):\n",
    "        self.weight_matrix = np.random.randn(num_inputs,1)\n",
    "        self.bias = 1\n",
    "\n",
    "    def _ReLu(z):\n",
    "        return max(0,z)\n",
    "\n",
    "    def _Sigmoid(z):\n",
    "        pass\n",
    "\n",
    "    def _tanh(z):\n",
    "        pass\n",
    "\n",
    "        \n",
    "    def forward(self,input,activation):\n",
    "        sum_total = np.dot(input,self.weight_matrix)+self.bias\n",
    "        print(sum_total)\n",
    "        if activation=='relu':\n",
    "            z = self._ReLu(sum_total)\n",
    "            return z\n",
    "        elif activation=='sigmoid':\n",
    "            pass\n",
    "        elif activation=='tanh':\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a09cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.07277046]\n",
      " [-2.302465  ]\n",
      " [-3.14554092]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Neuron._ReLu() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m neuron \u001b[38;5;241m=\u001b[39m Neuron(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m      4\u001b[0m                   [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m      5\u001b[0m                   [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m]])\n\u001b[0;32m----> 7\u001b[0m \u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mNeuron.forward\u001b[0;34m(self, input, activation)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(sum_total)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m activation\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ReLu\u001b[49m\u001b[43m(\u001b[49m\u001b[43msum_total\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m activation\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Neuron._ReLu() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "neuron = Neuron(2)\n",
    "\n",
    "input = np.array([[1,2],\n",
    "                  [2,3],\n",
    "                  [2,4]])\n",
    "\n",
    "neuron.forward(input=input,activation='relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30b05edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "[[-2.25574159  2.4502174   7.17718263 -1.10829226]]\n"
     ]
    }
   ],
   "source": [
    "num_neurons = 4\n",
    "num_input = 2\n",
    "demo_w = np.random.randn(num_input,num_neurons)\n",
    "demo_w.shape\n",
    "inp = np.array([[2,3]])\n",
    "\n",
    "print(inp.shape)\n",
    "abc = np.dot(inp,demo_w)\n",
    "print(abc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a76e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "\n",
    "    def __init__(self,num_neuron,num_inputs):\n",
    "        self.weights = np.random.randn(num_inputs,num_neuron)\n",
    "        self.bias = np.random.randn(1,num_neuron)\n",
    "\n",
    "\n",
    "    def forward(self,input_matrix):\n",
    "        print(\"Bias:\",self.bias)\n",
    "        sum = np.dot(input_matrix,self.weights)+self.bias\n",
    "        return sum\n",
    "\n",
    "    def fit(self,ephocs):\n",
    "\n",
    "        #loop till those ephocs\n",
    "        for i in range()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9190993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: [[-1.22496703  1.18861297 -0.18035877  1.18823604]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.92291336,  0.893657  , -6.03857776, -6.81407874]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = Dense(4,2)\n",
    "layer1.forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9489a998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0b932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
